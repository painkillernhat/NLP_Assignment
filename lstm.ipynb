{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyvi in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (0.1.1)\n",
      "Requirement already satisfied: keras in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "Requirement already satisfied: gensim in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (4.3.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from pyvi) (1.0.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from scikit-learn->pyvi) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from scikit-learn->pyvi) (3.2.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (0.9.9)\n",
      "Requirement already satisfied: six in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: tabulate in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (4.64.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "Collecting keras\n",
      "  Using cached keras-3.0.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: tensorflow in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from keras) (1.23.5)\n",
      "Collecting rich (from keras)\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Using cached namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: h5py in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from keras) (3.9.0)\n",
      "Collecting dm-tree (from keras)\n",
      "  Using cached dm_tree-0.1.8-cp39-cp39-macosx_11_0_arm64.whl (110 kB)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (69.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.56.2)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from rich->keras) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-text==2.13.* (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-text==2.13.*\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install pyvi keras gensim\n",
    "!pip3 install --upgrade keras tensorflow\n",
    "!pip3 install \"tensorflow-text==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "# from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./vlsp_sentiment_train.csv\", sep='\\t')\n",
    "data_train.columns =['Class', 'Data']\n",
    "data_test = pd.read_csv(\"./vlsp_sentiment_test.csv\", sep='\\t')\n",
    "data_test.columns =['Class', 'Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 2)\n",
      "(1050, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train.iloc[:, 0].values\n",
    "reviews = data_train.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stopwords.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "def remove_stop_words(text, stopwords):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "\n",
    "for label in labels:\n",
    "    if label == -1:\n",
    "        encoded_labels.append([1,0,0])\n",
    "    elif label == 0:\n",
    "        encoded_labels.append([0,1,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "\n",
    "encoded_labels = np.array(encoded_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_processed = []\n",
    "unlabeled_processed = [] \n",
    "for review in reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
    "    # review_cool_one = remove_stop_words(review_cool_one, stopwords)\n",
    "    reviews_processed.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyVi for Vietnamese word tokenizer\n",
    "word_reviews = []\n",
    "all_words = []\n",
    "for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(word_reviews)\n",
    "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (5100, 300)\n",
      "Shape of label train and validation tensor: (5100, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:',data.shape)\n",
    "print('Shape of label train and validation tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"./vi-model-CBOW.bin\", binary=True)\n",
    "\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 300, 400)          3167600   \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 300, 400)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 300, 2048)         11673600  \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 300, 1024)         10489856  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 512)               2623488   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27956083 (106.64 MB)\n",
      "Trainable params: 27956083 (106.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, LSTM\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "sequence_length = data.shape[1]\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
    "\n",
    "# Use bidirectional LSTM layers\n",
    "lstm_2 = Bidirectional(LSTM(1024, return_sequences=True))(reshape)\n",
    "lstm_1 = Bidirectional(LSTM(512, return_sequences=True))(lstm_2)\n",
    "lstm_0 = Bidirectional(LSTM(256))(lstm_1)\n",
    "\n",
    "dropout = Dropout(drop)(lstm_0)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 745s 47s/step - loss: 1.1938 - accuracy: 0.4289 - val_loss: 1.8158 - val_accuracy: 9.8039e-04 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 959s 61s/step - loss: 0.9620 - accuracy: 0.5760 - val_loss: 1.5777 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 1040s 66s/step - loss: 0.8834 - accuracy: 0.6218 - val_loss: 2.0583 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7271 - accuracy: 0.7159 \n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "16/16 [==============================] - 996s 63s/step - loss: 0.7271 - accuracy: 0.7159 - val_loss: 1.5821 - val_accuracy: 0.1863 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 1084s 68s/step - loss: 0.5860 - accuracy: 0.7890 - val_loss: 1.8774 - val_accuracy: 0.2343 - lr: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x290951b50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\n",
    "callbacks_list = [early_stopping, lr_reduction]\n",
    "\n",
    "model.fit(data, labels, validation_split=0.2,\n",
    "          epochs=10, batch_size=256, callbacks=callbacks_list, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test.iloc[:, 0].values\n",
    "reviews_test = data_test.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_test = []\n",
    "\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    elif label_test == 0:\n",
    "        encoded_labels_test.append([0,1,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "\n",
    "encoded_labels_test = np.array(encoded_labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_processed_test = []\n",
    "unlabeled_processed_test = [] \n",
    "for review_test in reviews_test:\n",
    "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
    "    # review_cool_one = remove_stop_words(review_cool_one, stopwords)\n",
    "    reviews_processed_test.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyVi for Vietnamese word tokenizer\n",
    "word_reviews_test = []\n",
    "all_words = []\n",
    "for review_test in reviews_processed_test:\n",
    "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
    "    word_reviews_test.append(review_test.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (1050, 300)\n",
      "Shape of label train and validation tensor: (1050, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:',data_test.shape)\n",
    "print('Shape of label train and validation tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 85s 3s/step - loss: 0.9188 - accuracy: 0.6362\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 91.88%\n",
      "accuracy: 63.62%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
