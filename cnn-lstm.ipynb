{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyvi in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (0.1.1)\n",
      "Requirement already satisfied: keras in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "Requirement already satisfied: gensim in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (4.3.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from pyvi) (1.0.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from scikit-learn->pyvi) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from scikit-learn->pyvi) (3.2.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (0.9.9)\n",
      "Requirement already satisfied: six in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: tabulate in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from sklearn-crfsuite->pyvi) (4.64.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "Collecting keras\n",
      "  Using cached keras-3.0.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: tensorflow in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from keras) (1.23.5)\n",
      "Collecting rich (from keras)\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Using cached namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: h5py in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from keras) (3.9.0)\n",
      "Collecting dm-tree (from keras)\n",
      "  Using cached dm_tree-0.1.8-cp39-cp39-macosx_11_0_arm64.whl (110 kB)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (69.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.56.2)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from rich->keras) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-text==2.13.* (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-text==2.13.*\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install pyvi keras gensim\n",
    "!pip3 install --upgrade keras tensorflow\n",
    "!pip3 install \"tensorflow-text==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "# from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./vlsp_sentiment_train.csv\", sep='\\t')\n",
    "data_train.columns =['Class', 'Data']\n",
    "data_test = pd.read_csv(\"./vlsp_sentiment_test.csv\", sep='\\t')\n",
    "data_test.columns =['Class', 'Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 2)\n",
      "(1050, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train.iloc[:, 0].values\n",
    "reviews = data_train.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stopwords.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "def remove_stop_words(text, stopwords):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "\n",
    "for label in labels:\n",
    "    if label == -1:\n",
    "        encoded_labels.append([1,0,0])\n",
    "    elif label == 0:\n",
    "        encoded_labels.append([0,1,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "\n",
    "encoded_labels = np.array(encoded_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_processed = []\n",
    "unlabeled_processed = [] \n",
    "for review in reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
    "    # review_cool_one = remove_stop_words(review_cool_one, stopwords)\n",
    "    reviews_processed.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyVi for Vietnamese word tokenizer\n",
    "word_reviews = []\n",
    "all_words = []\n",
    "for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(word_reviews)\n",
    "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (5100, 300)\n",
      "Shape of label train and validation tensor: (5100, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:',data.shape)\n",
    "print('Shape of label train and validation tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"./vi-model-CBOW.bin\", binary=True)\n",
    "\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras-self-attention\n",
      "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/thunguyen/Library/Python/3.9/lib/python/site-packages (from keras-self-attention) (1.23.5)\n",
      "Building wheels for collected packages: keras-self-attention\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=b3fc6640b379d5e892bd8bdb77dcf8d6887932faa1c4de606047e5da47e018bf\n",
      "  Stored in directory: /Users/thunguyen/Library/Caches/pip/wheels/78/c1/84/b83a2fd6f1d63e136cba74bac4126bee3b8705eef6486635fd\n",
      "Successfully built keras-self-attention\n",
      "Installing collected packages: keras-self-attention\n",
      "Successfully installed keras-self-attention-0.51.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)       [(None, 300)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_14 (Embedding)    (None, 300, 400)             3167600   ['input_21[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_52 (Conv1D)          (None, 298, 100)             120100    ['embedding_14[5][0]']        \n",
      "                                                                                                  \n",
      " conv1d_53 (Conv1D)          (None, 297, 100)             160100    ['embedding_14[5][0]']        \n",
      "                                                                                                  \n",
      " conv1d_54 (Conv1D)          (None, 296, 100)             200100    ['embedding_14[5][0]']        \n",
      "                                                                                                  \n",
      " max_pooling1d_25 (MaxPooli  (None, 1, 100)               0         ['conv1d_52[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_26 (MaxPooli  (None, 1, 100)               0         ['conv1d_53[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_27 (MaxPooli  (None, 1, 100)               0         ['conv1d_54[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenat  (None, 3, 100)               0         ['max_pooling1d_25[0][0]',    \n",
      " e)                                                                  'max_pooling1d_26[0][0]',    \n",
      "                                                                     'max_pooling1d_27[0][0]']    \n",
      "                                                                                                  \n",
      " lstm_18 (LSTM)              (None, 128)                  117248    ['concatenate_24[0][0]']      \n",
      "                                                                                                  \n",
      " attention_7 (Attention)     (None, 128)                  0         ['lstm_18[0][0]',             \n",
      "                                                                     'lstm_18[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 128)                  0         ['attention_7[0][0]']         \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 3)                    387       ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3765535 (14.36 MB)\n",
      "Trainable params: 3765535 (14.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Reshape, Conv1D, MaxPooling1D, Dropout, LSTM, Dense, concatenate, Attention, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "input = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(input)\n",
    "reshape = Reshape((sequence_length, EMBEDDING_DIM))(embedding)\n",
    "\n",
    "conv_0 = Conv1D(num_filters, filter_sizes[0], activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
    "maxpool_0 = MaxPooling1D(sequence_length-filter_sizes[0]+1, strides=1)(conv_0)\n",
    "\n",
    "conv_1 = Conv1D(num_filters, filter_sizes[1], activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
    "maxpool_1 = MaxPooling1D(sequence_length-filter_sizes[1]+1, strides=1)(conv_1)\n",
    "\n",
    "conv_2 = Conv1D(num_filters, filter_sizes[2], activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
    "maxpool_2 = MaxPooling1D(sequence_length-filter_sizes[2]+1, strides=1)(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2])\n",
    "# merged_tensor = Reshape((-1, 1))(merged_tensor) \n",
    "lstm_2 = LSTM(units=128, return_sequences=False)(merged_tensor)\n",
    "\n",
    "# attention = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                            #  kernel_regularizer=regularizers.l2(0.01))(lstm_2)\n",
    "\n",
    "attention = Attention()([lstm_2, lstm_2])\n",
    "\n",
    "attended_output = Concatenate(axis=-1)([lstm_2, attention])\n",
    "\n",
    "dropout = Dropout(rate=drop)(attention)\n",
    "output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "            metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 15s 867ms/step - loss: 4.6465 - accuracy: 0.3350 - val_loss: 3.2397 - val_accuracy: 0.3510\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 13s 811ms/step - loss: 2.5030 - accuracy: 0.3311 - val_loss: 1.8504 - val_accuracy: 0.4049\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 13s 834ms/step - loss: 1.5492 - accuracy: 0.4020 - val_loss: 1.2881 - val_accuracy: 0.4725\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 14s 884ms/step - loss: 1.1967 - accuracy: 0.4961 - val_loss: 1.1411 - val_accuracy: 0.4784\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 14s 862ms/step - loss: 1.0963 - accuracy: 0.5203 - val_loss: 1.1365 - val_accuracy: 0.4559\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 15s 935ms/step - loss: 1.0238 - accuracy: 0.5395 - val_loss: 1.1039 - val_accuracy: 0.4941\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 14s 892ms/step - loss: 0.9714 - accuracy: 0.5534 - val_loss: 1.1253 - val_accuracy: 0.4951\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 14s 895ms/step - loss: 0.9117 - accuracy: 0.6012 - val_loss: 1.1465 - val_accuracy: 0.5078\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 14s 862ms/step - loss: 0.9479 - accuracy: 0.5824 - val_loss: 1.2681 - val_accuracy: 0.4363\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 14s 883ms/step - loss: 0.9680 - accuracy: 0.5956 - val_loss: 1.1862 - val_accuracy: 0.5216\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29f5c1670>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size=256, callbacks=callbacks_list)\n",
    "# model.save_weights(\"./model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test.iloc[:, 0].values\n",
    "reviews_test = data_test.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_test = []\n",
    "\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    elif label_test == 0:\n",
    "        encoded_labels_test.append([0,1,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "\n",
    "encoded_labels_test = np.array(encoded_labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_processed_test = []\n",
    "unlabeled_processed_test = [] \n",
    "for review_test in reviews_test:\n",
    "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
    "    # review_cool_one = remove_stop_words(review_cool_one, stopwords)\n",
    "    reviews_processed_test.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyVi for Vietnamese word tokenizer\n",
    "word_reviews_test = []\n",
    "all_words = []\n",
    "for review_test in reviews_processed_test:\n",
    "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
    "    word_reviews_test.append(review_test.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (1050, 300)\n",
      "Shape of label train and validation tensor: (1050, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:',data_test.shape)\n",
    "print('Shape of label train and validation tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 30ms/step - loss: 0.9744 - accuracy: 0.6971\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 97.44%\n",
      "accuracy: 69.71%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
